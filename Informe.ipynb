{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Estructura de Datos</center>\n",
    "## <center>Universidad Nacional de Tres de Febrero</center>\n",
    "## <center>TRZearcher</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRABAJO PRACTICO - TRANZAS DEL CODIGO\n",
    "    Aguilera, Agustin\n",
    "    Ambrosetti, Ramiro\n",
    "    Noguera, Sol\n",
    "    Tabarovsky, Javier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRZearcher\n",
    "    Es un programa de busqueda y comparacion de precios de productos tecnologicos, para encontrar el mejor precio en el menor tiempo posible\n",
    "    Por el momento trabajamos con: \n",
    "        - Compragamer\n",
    "        - Full H4rd\n",
    "        - Gezatek\n",
    "        - Overdrive\n",
    "        - Venex\n",
    "    Estas paginas fueron seleccionadas debido a que son las mas economicas en su rubro a la fecha de publicacion de este informe, son paginas en continuo crecimiento, actualizadas diariamente, y con la mayor variedad de productos.\n",
    "    \n",
    "    Video presentacion: https://www.youtube.com/watch?v=i_qlmObgpmU&t=12s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UML - Diagrama de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El UML esta en formato pdf para ahorrar espacio y subirlo a la plataforma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede encontrarse en este link para una mejor visualizacion:\n",
    "    https://app.lucidchart.com/invitations/accept/5984dc3f-ae4b-4041-a64c-ddd82dfaeefc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TECNICAS DE PROGRAMACION Y DISEÑO\n",
    "    Las tecnicas de programacion utilizadas en este proyecto se basan en la programacion orientada a objetos, y la utilizacion de la libreria Scrapy.\n",
    "    El diseño del programa esta dividido en 2 procesos, el Front y Back, esto se realizo de esta manera para solventar un problema.\n",
    "    El problema se centraba en que al ejecutarse el programa, primero las spiders leian la informacion de configuracion y luego se ejecutaba la interfaz, esto ocasionaba que las url de inicio se encontraban incompletas\n",
    "    El diseño se basa en la modularizacion de las funcionalidades, a continuacion se puede ver un diagrama donde explicamos como se desenvuelve el programa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Ilustracion.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El programa comienza su ejecucion desde la clase Main, la cual llama a Interface y la ejecuta:\n",
    "\n",
    "    1 - Una vez que el usuario eligio su configuracion de busqueda, y presiona el boton de buscar, estas configuraciones son guardadas en un temporal denominado temp_connector, y otro temporal con las url a las paginas completas denominado pages_complete.\n",
    "    2 - Este temp_conector es leido por la clase Proyect Connector.                           \n",
    "    3 - La clase Proyect Connector, instancia a la clase Spider Connector la cual se encarga de :\n",
    "        3.1 - Instanciar a la clase Orchestrator, la cual ejecuta las spiders seleccionadas, y estas mismas exportan los datos en temporales.csv.\n",
    "        3.2 - Instanciar a la clase Sorter, la cual lee los temporales.csv, los ordena y filtra segun el tipo de busqueda, para luego instanciar a la clase Exporter con los resultados.\n",
    "    4 - La clase Exporter se encarga de escribir los resultados en dos formatos, csv, json y html.\n",
    "    Esta seria una explicacion a grandes rasgos del funcionamiento del programa         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERFACE (FRONT)\n",
    "    La interfaz esta creada como un proceso separado del programa, con una unica clase la cual crea una interfaz grafica con el uso de la libreria tkinter.\n",
    "    En la cual a la hora de presionar el boton de buscar se realizan las siguientes acciones:\n",
    "    \n",
    "        1. Verifica que la entrada del producto no puede estar vacia, y al menos se debe seleccionar una pagina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __code_button(self):\n",
    "            if (str(self.product.get()).strip() == \"\") and (self.compra_gamer.get() + self.full_hard.get() + self.gezatek.get() + self.venex.get() + self.overdrive.get() <= 0):\n",
    "                messagebox.showerror(message=\"Recordar que:\\n - La entrada del producto no puede estar vacia\\n - Al menos hay que seleccionar una pagina\", title=\"Problema con TRZearcher\")\n",
    "            else:\n",
    "                self.__write_data_connector()\n",
    "                self.__write_pages()\n",
    "                subprocess.call(\"python ./Back/Connector/proyect_connector.py\", shell=True)\n",
    "                self.window.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2. Escribe los datos de la interfaz en un temporal, llamado temp_connector.txt, el cual luego es leido por el otro proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __write_data_connector(self):\n",
    "        with open(\"./Back/Connector/temp_connector.txt\", 'w') as pages_file:\n",
    "            pages_file.write(str(self.product.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.compra_gamer.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.full_hard.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.gezatek.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.venex.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.overdrive.get()) + \"\\n\")\n",
    "            pages_file.write(str(self.option.get()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3. Completa las paginas a buscar, leyendo de pages_initials.txt, agregando el producto al final y lo exporta a un temporal llamado page_complete.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __write_pages(self):\n",
    "        pages_complete = []\n",
    "        number_of_pages = 0\n",
    "        fileInitials = os.path.abspath(\"./Back/Searcher/Data/pages_initials.txt\")\n",
    "        fileComplete = os.path.abspath(\"./Back/Searcher/Data/pages_complete.txt\")\n",
    "        with open(fileInitials, \"r\") as pages_file:\n",
    "            for page in pages_file:\n",
    "                number_of_pages += 1\n",
    "                page = page.rstrip('\\n') + str(self.product.get()) + \"\\n\"\n",
    "                pages_complete.append(page)\n",
    "        with open(fileComplete, \"w\") as pages_file:\n",
    "            for x in range(0, number_of_pages):\n",
    "                pages_file.write(pages_complete[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Una vez finalizado, se ejecuta el siguiente proceso, y se cierra la ventana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECTOR (BACK)\n",
    "    El modulo connector esta creado para la conexion del programa, la cual esta compuesta de dos clases proyect_connector y spyder_connector.\n",
    "        1. proyect_connector: Se basa en la lectura de los datos obtenidos de interfaz (a travez del temporal temp_connector.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    __data = []\n",
    "    with open(\"./Back/Connector/temp_connector.txt\", \"r\") as data_connector:\n",
    "        for data in data_connector:\n",
    "            __data.append(str(data).strip())\n",
    "\n",
    "    Connector(__data[0], __data[1], __data[2], __data[3], __data[4], __data[5], __data[6]).execute_connector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2. spyder_connector: Es la clase mas importante del programa, la cual es la encargada en primer instancia de verificar que paginas se quieren buscar, para proceder a ejecutar las Spiders, a su vez elimina los archivos temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Connector:\n",
    "\n",
    "        def __init__(self, product, compragamer, fullhard, gezatek, venex, overdrive, type_search):\n",
    "            self.product = product\n",
    "            self.pages = self.__spiders_validation(compragamer, fullhard, gezatek, venex, overdrive)\n",
    "            self.type_search = type_search\n",
    "\n",
    "        def __spiders_validation(self, compragamer, fullhard, gezatek, venex, overdrive):\n",
    "            pages = []\n",
    "            if int(compragamer) == 1:\n",
    "                pages.append(\"compragamer\")\n",
    "            if int(fullhard) == 1:\n",
    "                pages.append(\"fullh4rd\")\n",
    "            if int(gezatek) == 1:\n",
    "                pages.append(\"gezatek\")\n",
    "            if int(overdrive) == 1:\n",
    "                pages.append(\"overdrive\")\n",
    "            if int(venex) == 1:\n",
    "                pages.append(\"venex\")\n",
    "            return pages\n",
    "\n",
    "        def execute_connector(self):\n",
    "            Orchestrator().execute_spiders(self.pages)\n",
    "            Sorter(self.pages, self.product, self.type_search).execute_sorter()\n",
    "            self.__clean()\n",
    "            sys.exit()\n",
    "\n",
    "        def __clean(self):\n",
    "            os.remove(\"./Back/Connector/temp_connector.txt\")\n",
    "            os.remove(\"./Back/Searcher/Data/pages_complete.txt\")\n",
    "            for page in self.pages:\n",
    "                os.remove(page + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESSOR (BACK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modulo processor tiene dos clases: Sorter y Exporter.\n",
    "Este el encargado de ordenar y organizar los datos levantados de la web por el modulo searcher y escribirlos en distintos\n",
    "formatos para la visualización de el resultado de la busqueda.\n",
    "\n",
    "  ### Clase Sorter:\n",
    "\n",
    "        Clase que a partir de archivos .csv de cada pagina con toda la informacion que el crawler levanto\n",
    "        de la web, los lee y organiza para luego llamar a la clase Export, y escribir la salida.\n",
    "\n",
    "Para hacer esto, se necesitan los inputs del usuario (lo que introdujo en la busqueda, las paginas en las que\n",
    "desea buscar, y el tipo de busqueda), lo cual se ingresa en el constructor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este metodo es el que junta todos los pasos necesarios para cumplir el objetivo de la clase (ordenar y llamar a export):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def execute_sorter(self):\n",
    "        self.__read_data()\n",
    "        self.__sort_by_price(self.exact_words)\n",
    "        self.__index_dict()\n",
    "        self.__export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo read data lee los archivos .csv con categoria, precio, titulo y lo agrega a dos listas de\n",
    "listas del tipo [\"Page\", \"Category\", \"Title\", \"Price\", \"Link\", \"Time\"]. Una lista exact_words contiene \n",
    "las que tienen titulo exactamente igual y not_exact_words el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def __sort_by_price(self, products_list):\n",
    "        products_list.sort(key=lambda e: e[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordena la lista de listas, itera y segun el elemento en la posicion 3 (que es el precio) lo reposiciona en la lista la cual se le pasa por parametro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __index_dict(self):\n",
    "        self.__create_lists_in_dict()\n",
    "        words = self.product_to_search.split()\n",
    "        for product in self.not_exact_words:\n",
    "            separated_title = product[2].split()\n",
    "            counter = 0\n",
    "            for word in words:\n",
    "                if word in separated_title:\n",
    "                    counter += 1\n",
    "\n",
    "            self.matching_words_to_product[counter].append(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea el diccionario matching_words_to_product el cual indexa con clave de la cantidad de palabras\n",
    "     que coinciden entre el titulo y lo que se busco. El valor son las listas de productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __export(self):\n",
    "        \"\"\"\n",
    "        Segun el tipo de busqueda elegida, se llamara a una instancia Export con los parametros correspondientes\n",
    "        para que escriba el .csv con los datos ordenados.\n",
    "        \"\"\"\n",
    "\n",
    "        separated_words = self.product_to_search.split()\n",
    "        if self.search_type == \"FRASE_EXACTA\":\n",
    "            exporter = Exporter(self.product_to_search, self.exact_words)\n",
    "            self.__write(exporter)\n",
    "\n",
    "        elif self.search_type == \"CONTIENE_TODAS_LAS_PALABRAS\":\n",
    "            products_all_words = self.matching_words_to_product[len(separated_words)]\n",
    "            self.__sort_by_price(products_all_words)\n",
    "            products_all_words = self.exact_words + products_all_words\n",
    "            exporter = Exporter(self.product_to_search, products_all_words)\n",
    "            self.__write(exporter)\n",
    "\n",
    "        elif self.search_type == \"CONTIENE_ALGUNAS_PALABRAS\":\n",
    "            products_some_words = list()\n",
    "            self.__collect_products(products_some_words, range(len(separated_words)))\n",
    "            self.__sort_by_price(products_some_words)\n",
    "            products_some_words = self.exact_words + products_some_words\n",
    "            exporter = Exporter(self.product_to_search, products_some_words)\n",
    "            self.__write(exporter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Este metodo se encarga de exportar los datos segun el tipo de busqueda elegida. Si se eligio frase exacta, simplemente\n",
    "usa la lista exact_words para pasarsela a Export y escribir en csv json y html.\n",
    "\n",
    "Si se eligio frase que contenga todas las palabras, busca en el diccionario que indexamos con anterioridad, la lista de productos que tienen la misma cantidad de coincidencias que numero de palabras se introdujo y la ordenamos. Luego, no podemos olvidarnos de las palabras exactamente iguales. Por lo que concatenamos la exacta (para que salga primero) con la lista que recibimos del diccionario.\n",
    "Llamamos a Exporter con esa lista y escribimos en csv, json y html.\n",
    "\n",
    "Si se eligio frase que contiene algunas palabras, el procedimiento es igual al anterior, pero en este caso, llama al metodo collect_products para que junte en una lista a todas las palabras que coinciden en una palabra, luego en dos, y asi hasta llegar a la cantidad de palabras totales que se introdujo por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __collect_products(self, products_list, amount):\n",
    "        for x in amount:\n",
    "            products_list += self.matching_words_to_product[x + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Junta en una lista a todos los productos que contengan algunas de las palabras en su titulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Clase Exporter:\n",
    "        Se encarga de escribir en diferentes formatos los productos que se desea visualizar el precio más economico.\n",
    "        Recibe los datos ordenados por Sorter.\n",
    "        Es capaz de escribir en: .csv, .html, .json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El constructor recibe la lista ordenada de productos, y ademas lo que ingreso el usuario para buscar, de manera que con eso el nombre del archivo es del estilo:\n",
    "\n",
    "         input_dia-mes-año.formato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 3 metodos publicos disponibles:\n",
    "###### write_csv\n",
    "Escribe un .csv con un csv.writer(), el cual escribe cada fila con un producto.\n",
    "\n",
    "###### write_html\n",
    "Escribe un html gracias a un string, el cual primero tiene el principio general de nuestro html con el framework de css bootstrap importado para una mejor estetica de la pagina. Luego recorre la lista de productos y escribe bloques de este estilo para cada producto:\n",
    "\n",
    "            <tr>\n",
    "                <th scope=\"row\">Categoria</th>\n",
    "                <td>\n",
    "                    <a href=\"link\">Titulo</a>\n",
    "                </td>\n",
    "                <td>Precio</td>\n",
    "                <td>Fecha</td>\n",
    "            </tr>\n",
    "            \n",
    "Concatena todos los bloques al string base, y despues de terminar de recorrer los productos, se concatena un string final para cerrar el html, y escribe el archivo con la cadena de caracteres final.\n",
    "\n",
    "###### write_json \n",
    "Con cada producto que viene en el formato [\"Page\", \"Category\", \"Title\", \"Price\", \"Link\", \"Time\"], por ejemplo \n",
    "     \n",
    "     [\"Gezatek\", \"Auriculares\", \"Auriculares Samsung\", \"3055\", \"https://gezatek.com/auriculares-samsung\", \"20:27\"]\n",
    "Lo transforma en un diccionario del estilo:\n",
    "    \n",
    "    {\"Page\": \"Gezatek\", \"Category\": \"Auriculares\", \"Title\": \"Auriculares Samsung\", \"Price\": \"3055\", \"Link\": \"https://gezatek.com/auriculares-samsung\", \"Time\": \"20:27\" }\n",
    "\n",
    "Esto se realiza para ser interpretado por el metodo dump que nos proporciona la libreria json.\n",
    "Lo que hace json.dump es recibir un diccionario y escribirlo en formato json, asi que abrimos el nuevo archivo y escribimos con dump un diccionario final con clave \"products\" y valor una lista de diccionarios de cada producto del estilo anteriormente mostrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCHER (BACK)\n",
    "    El modulo searcher es el encargado de recuperar los datos de la web, se divide a su vez en:\n",
    "        1. Config: Configuracion de scrapy, con sus items, etc.\n",
    "        2. Data: Archivos temporales y estaticos, los cuales son utilizados por las arañas para su funcionamiento.\n",
    "        3. Spiders: Son las encargadas de recuperar la informacion de determinadas paginas, y exportarlas a csv temporales. Este modulo incluye su orchestrator, el cual las ejecuta.\n",
    "        Un ejemplo podria ser la CompragamerSpider, las demas son similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    La primer parte de cada spider, se basa en su configuracion:\n",
    "        1. name: Nombre de la araña.\n",
    "        2. custom_settings: Forma de exportacion de los datos, en este caso csv.\n",
    "        3. allowed_domains: Referencia al dominio en el cual se va a trabajar, del cual no se puede salir.\n",
    "        4. item_count: Es una variable que utilizaremos mas adelante para limitar la salida de datos, y que el programa no tarde demasiado tiempo.\n",
    "        5. start_urls: Links de las paginas desde donde se va a lanzar a recolectar datos, en este caso se toman desde un archivo temporal.\n",
    "        6. rules: Reglas que la spider debe tener en cuenta en este caso, es que cada vez que encuentre un elemento de la lista (xpaths), entre a el dominio ejecutando parse_items,\n",
    "            esto es muy util ya que al entrar a cada publicacion se puede extraer informacion que no se encuentra en la lista de la url incial. Otra regla podria ser que cada vez que encuentre\n",
    "            un xpaths de un boton (por ejemplo siguiente pagina), siga esa url, para continuar con la busqueda.\n",
    "        Por ultimo elimina el archivo temporal anterior, en el caso de que todavia siga existiendo (esto se hace si por alguna razon el programa se interrumpe en su proceso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class CompragamerSpider(CrawlSpider):\n",
    "        \"\"\"\n",
    "            Spider que recolecta datos de la pagina www.compragamer.com, con un limite de 15\n",
    "        \"\"\"\n",
    "\n",
    "        name = 'compragamer'\n",
    "\n",
    "        custom_settings = {'FEEDS': {'compragamer.csv': {'format': 'csv'}}}\n",
    "\n",
    "        allowed_domains = ['compragamer.com']\n",
    "\n",
    "        item_count = 0\n",
    "\n",
    "        url = \"\"\n",
    "        filePath = os.path.abspath(\"../TRZearcher/Back/Searcher/Data/pages_complete.txt\")\n",
    "        with open(filePath, \"r\") as pages:\n",
    "            for page in pages:\n",
    "                if page.find(\"compragamer\") > 0:\n",
    "                    url = page\n",
    "\n",
    "        start_urls = [url]\n",
    "\n",
    "        rules = {\n",
    "            Rule(LinkExtractor(allow=(), restrict_xpaths=('//*[@class=\"products__item\"]')),\n",
    "                 callback='parse_item', follow=False)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            os.remove('compragamer.csv')\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    La segunda parte de cada spider, se basa en la recuperacion de datos:\n",
    "        - La llamada al TrzpidersItem se da para guardar los elementos que vamos a recuperar.\n",
    "        - Cada elemento se recupera en mediante css (menos link y time), y dependiendo como se encuentre se arregla para su posterior salida en csv.\n",
    "        - Los elementos a extraer por pagina se limitan a un maximo de 15 elementos, para que el programa no tarde demasiado tiempo y recupere muchos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def parse_item(self, response):\n",
    "                \"\"\"\n",
    "                    Recolecta la informacion de cada item\n",
    "                \"\"\"\n",
    "\n",
    "                item = TrzpidersItem()\n",
    "\n",
    "                item['title'] = str(response.css(\n",
    "                    '.filaNombre div::text').extract_first()).strip()\n",
    "\n",
    "                item['price'] = str(response.css(\n",
    "                    '.col-xs-5::text').extract_first()).split(\"$\")[1].strip()\n",
    "\n",
    "                item['category'] = str(response.css(\n",
    "                    '.detalleNombre .product-card__name::text').extract_first()).capitalize().strip()\n",
    "\n",
    "                item['link'] = str(response.url)\n",
    "\n",
    "                item['time'] = (str(datetime.now().year) + \"-\" + str(datetime.now().month) + \"-\" + str(\n",
    "                    datetime.now().day) + \" \" + str(datetime.now().hour) + \":\" + str(datetime.now().minute))\n",
    "\n",
    "                self.item_count += 1\n",
    "                if self.item_count > 15:\n",
    "                    raise CloseSpider('item exceeded')\n",
    "\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST (BACK)\n",
    "    Para testear las clases, se decidió conjuntamente no hacer las clásicas pruebas unitarias, ya que el código es complejo y no todos los métodos retornan un valor.\n",
    "    El profesor asignado nos sugirió que la clase test heredara a la clase Sorter pero, por alguna razón, no permitía importar los casos de testeo.\n",
    "\n",
    "    Para los métodos específicamente se suprimieron las partes que llamaban a otro objeto para simplificar el código, ya que lo se buscaba testear era que los métodos pasaran correctamente los filtros.\n",
    "    Estas partes fueron reemplazadas por valores a retornar para poder ejecutar pruebas unitarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class TestSorter(unittest.TestCase):\n",
    "        search_type = str\n",
    "\n",
    "        def test_chooses_search_type_correctly(self):\n",
    "            self.assertEqual(self.__define_search_type(1), \"FRASE_EXACTA\")\n",
    "            self.assertEqual(self.__define_search_type(2), \"CONTIENE_TODAS_LAS_PALABRAS\")\n",
    "            self.assertEqual(self.__define_search_type(3), \"CONTIENE_ALGUNAS_PALABRAS\")\n",
    "\n",
    "        def test_normalizes_price(self):\n",
    "            self.assertEqual(self.__normalize_price(\"$21.00\"), '$21')\n",
    "\n",
    "        def test_exports_correctly(self):\n",
    "            self.search_type = \"FRASE_EXACTA\"\n",
    "            self.assertEqual(self.__export(), 'Exportado frase exacta')\n",
    "            self.search_type = \"CONTIENE_TODAS_LAS_PALABRAS\"\n",
    "            self.assertEqual(self.__export(), 'Exportado contiene todas las palabras')\n",
    "            self.search_type = \"CONTIENE_ALGUNAS_PALABRAS\"\n",
    "            self.assertEqual(self.__export(), 'Exportado contiene algunas palabras')\n",
    "\n",
    "        def test_sort_by_price(self):\n",
    "            products_list = []\n",
    "            with open(\"testpage.csv\", 'r') as f:\n",
    "                file = csv.DictReader(f, delimiter=\",\")\n",
    "\n",
    "                for line in file:\n",
    "                    product = [line[\"Page\"], line[\"Category\"].lower(), line[\"Title\"].lower(), int(line[\"Price\"]), line[\"Link\"],\n",
    "                               line[\"Time\"]]\n",
    "                    products_list.append(product)\n",
    "            products_list_sorted = products_list.sort(key=lambda e: e[3])\n",
    "            self.assertEqual(self.__sort_by_price(products_list), products_list_sorted)\n",
    "\n",
    "\n",
    "\n",
    "        def __sort_by_price(self, products_list):\n",
    "            products_list.sort(key=lambda e: e[3])\n",
    "        def __export(self):\n",
    "\n",
    "            if self.search_type == \"FRASE_EXACTA\":\n",
    "                return 'Exportado frase exacta'\n",
    "\n",
    "            elif self.search_type == \"CONTIENE_TODAS_LAS_PALABRAS\":\n",
    "                return 'Exportado contiene todas las palabras'\n",
    "\n",
    "            elif self.search_type == \"CONTIENE_ALGUNAS_PALABRAS\":\n",
    "                return 'Exportado contiene algunas palabras'\n",
    "        def __normalize_price(self, price):\n",
    "            price = price.replace('\"', \"\")\n",
    "            price = price.replace(',', \"\")\n",
    "            price = price.replace('.00', \"\")\n",
    "            price = price.replace('.', \"\")\n",
    "            return price\n",
    "        def __define_search_type(self,search_type):\n",
    "            if int(search_type) == 1:\n",
    "                return \"FRASE_EXACTA\"\n",
    "            if int(search_type) == 2:\n",
    "                return \"CONTIENE_TODAS_LAS_PALABRAS\"\n",
    "            if int(search_type) == 3:\n",
    "                return \"CONTIENE_ALGUNAS_PALABRAS\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
